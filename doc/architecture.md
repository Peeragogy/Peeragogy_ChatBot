# ğŸ“± PeeragogyBot v1 â€“ Flow Architecture (Technical Overview)

> â€œLearning is co-created, not consumed. And AI? Just another peer in the loop.â€  
> â€” FTG-003 & Gino

## ğŸ§  Project Overview

This README documents the internal architecture of the first production-ready version of the **PeeragogyBot**, an AI-powered peer learning assistant built using **FlowiseAI**. The bot is designed to facilitate real-time, context-aware conversations aligned with **Pyragogy principles** â€“ empowering collaborative, ethical, and joyful exploration of knowledge.

---

## âš™ï¸ How the PeeragogyBot Pipeline Works

The architecture of `PeeragogyBot` is built as a modular pipeline in Flowise, combining natural language input, contextual embedding retrieval, memory management, and LLM orchestration. Hereâ€™s a breakdown of how it all works:

---

### 1. ğŸ› User Prompt Ingestion

The pipeline begins with a **Chat Input Node** that captures the userâ€™s message. This node acts as the entry point and passes the message downstream for processing.

- **Input**: Free-text question or request.
- **Output**: Raw user input â†’ sent to memory + prompt nodes.

---

### 2. ğŸ§  Memory & Context Assembly

To ensure coherent dialogue across multiple interactions, the system uses a **Chat History Node** that appends past user and bot exchanges.

- Enables *contextual continuity* across turns.
- Memory is currently ephemeral but can connect to a persistent store (e.g., MongoDB or NeonDB).

---

### 3. ğŸ§„ Prompt Formatting (System Behavior)

Next, the user input and memory are routed to a **Prompt Template Node** which wraps the message in a system prompt. This defines the botâ€™s persona and tone.

- Sets PeeragogyBotâ€™s identity as a reflective, empathetic AI tutor.
- Example: _â€œYou are an AI peer-tutor helping the user explore knowledge collaboratively...â€_

---

### 4. ğŸ“š Knowledge Injection via Embedding Retrieval

Simultaneously, the original input is passed to a **Vector Search Node** which queries a precomputed embedding database (based on Pyragogy docs, Handbooks, etc.).

- Retrieves the top matching chunks semantically relevant to the prompt.
- These chunks are injected into the final prompt to inform the LLM.

---

### 5. ğŸ” Prompt Fusion

Both the behavioral prompt (persona + memory) and the retrieved knowledge chunks are merged into a final message format, ready to be consumed by the LLM.

- Ensures the response is both **contextual** (from memory) and **grounded** (from docs).

---

### 6. ğŸ¤– LLM Response Generation

The composed prompt is passed to the **LLM Node** (e.g., OpenAI GPT-4 or local Ollama models like Mistral).

- The LLM generates a pedagogically-tuned response based on prompt structure, memory, and retrieved context.
- Temperature and model parameters are optimized for clarity and reflectiveness.

---

### 7. ğŸ§¾ Output Delivery

Finally, the generated text is sent back to the user via the **Chat Output Node**.

- In the UI, this can be rendered as markdown.
- Future versions may include: source citations, typing indicators, and feedback buttons.

---

## ğŸ§¬ Pipeline Logic Summary (Nerd Mode)

```plaintext
User Prompt
   â†“
[ Memory Node ] â‡„ [ Prompt Template ]
   â†“                   â†“
[ Vector Search ] â€”â†’ Context Chunks
         â†“             â†“
     Final Prompt â€”â†’ [ LLM Node ] â€”â†’ Bot Reply
                               â†“
                       [ Output to User ]

